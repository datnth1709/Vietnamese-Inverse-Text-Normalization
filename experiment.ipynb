{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011954784393310547,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 702,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a3bcc4f08840b7a834942ba8a400e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/702 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014901399612426758,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 283095275,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9370a4b4a37b4b5299d52e839af44ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/270M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nguyenvulebinh/envibert were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at nguyenvulebinh/envibert and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at nguyenvulebinh/envibert and are newly initialized: ['roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011971712112426758,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 622703651,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787b587b3fc04dda9c0ad14efb13592f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/594M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from transformers.file_utils import cached_path, hf_bucket_url\n",
    "from importlib.machinery import SourceFileLoader\n",
    "from transformers import EncoderDecoderModel\n",
    "import argparse\n",
    "\n",
    "cache_dir = './cache'\n",
    "model_name = 'nguyenvulebinh/envibert'\n",
    "\n",
    "def download_tokenizer_files():\n",
    "    resources = ['envibert_tokenizer.py', 'dict.txt', 'sentencepiece.bpe.model']\n",
    "    for item in resources:\n",
    "        if not os.path.exists(os.path.join(cache_dir, item)):\n",
    "            tmp_file = hf_bucket_url(model_name, filename=item)\n",
    "            tmp_file = cached_path(tmp_file, cache_dir=cache_dir)\n",
    "            os.rename(tmp_file, os.path.join(cache_dir, item))\n",
    "\n",
    "def init_tokenizer():\n",
    "    download_tokenizer_files()\n",
    "    tokenizer = SourceFileLoader(\"envibert.tokenizer\",\n",
    "                                 os.path.join(cache_dir,\n",
    "                                              'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)\n",
    "    return tokenizer\n",
    "\n",
    "def init_model():\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "    download_tokenizer_files()\n",
    "    tokenizer = SourceFileLoader(\"envibert.tokenizer\",\n",
    "                                 os.path.join(cache_dir,\n",
    "                                              'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)\n",
    "    # set encoder decoder tying to True\n",
    "    roberta_shared = EncoderDecoderModel.from_encoder_decoder_pretrained(model_name,\n",
    "                                                                         model_name,\n",
    "                                                                         tie_encoder_decoder=False)\n",
    "\n",
    "    # set special tokens\n",
    "    roberta_shared.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "    roberta_shared.config.eos_token_id = tokenizer.eos_token_id\n",
    "    roberta_shared.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # sensible parameters for beam search\n",
    "    # set decoding params\n",
    "    roberta_shared.config.max_length = 100\n",
    "    roberta_shared.config.early_stopping = True\n",
    "    roberta_shared.config.no_repeat_ngram_size = 3\n",
    "    roberta_shared.config.length_penalty = 2.0\n",
    "    roberta_shared.config.num_beams = 1\n",
    "    roberta_shared.config.vocab_size = roberta_shared.config.encoder.vocab_size\n",
    "\n",
    "    return roberta_shared, tokenizer\n",
    "\n",
    "trained_model, tokenizer = init_model()\n",
    "trained_model = trained_model.from_pretrained(\"datnth1709/VietAI-NLP-ITN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text input: \n",
      "thời báo kinh tế sài gòn online bộ trưởng âm chín chín phẩy chín tám bộ giao thông vận tải hồ nghĩa dũng đã ký quyết định phê duyệt quy hoạch sân tám trăm tám mươi chín lít bay tại tỉnh an giang giai đoạn đến mười bốn giờ năm mươi ba và định hướng đến mồng một tháng bảy có tổng nhu cầu vốn dự kiến là tám chín ba sáu tám một ba bẩy bốn năm bốn tỉ đồng bình nguyên dự án sân bay tại xã cần đăng huyện châu thành sẽ được thực hiện thành một ngàn tám trăm ba mươi bẩy giai đoạn với số vốn cần cho giai đoạn đến bốn giờ không phút là âm tám tám chấm không năm mươi tỉ đồng và giai đoạn định hướng đến một nghìn sáu trăm là âm chín ba ngàn hai trăm năm bốn phẩy bốn nghìn chín mươi bẩy tỉ đồng theo quyết định phê duyệt quy hoạch số i gi gi rờ lờ chín không không đến ngày mười hai sân bay an giang sẽ có một đường hạ cất cánh dài hai mươi ba ngàn bẩy trăm mười bốn phẩy ba nghìn tám trăm sáu mươi chín mét và rộng cộng bốn hai sáu sáu sáu không năm bốn sáu một mét đảm bảo cho các hoạt động khai thác của máy bay chín trăm một trăm hai mươi trừ rờ đờ ngang gi hoặc tương đương đường băng sẽ được nâng cấp để khả năng tiếp nhận máy bay airbus hai trăm gạch chéo tám một không xuộc đê quờ o e dét cho giai đoạn sau ngày bẩy dự kiến lượng hành khách tại sân bay này là sáu triệu một trăm bốn mươi bảy ngàn bốn trăm năm mươi hai khách/giờ cao điểm và lượng hàng hóa là hai triệu không ngàn hai trăm tấn/năm ngoài ra dự án sân bay có tổng diện tích hơn hai nhăm héc-ta này cũng sẽ bao gồm nhà dịch vụ hành khách rộng sáu trăm năm mươi chín mi li mét vuông trung tâm điều hành bay ngày hai mươi bảy và ngày hai mươi chín trạm xe dẫn đường và các công trình kỹ thuật hỗ trợ cho các hoạt động hạ cất cánh đường sân đỗ ô tô trước nhà ga sau không giờ nhà ga sẽ được nâng cấp và mở rộng đến năm mươi ba xen ti mét trên xen ti mét khối\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text predict: \n",
      "thời báo kinh tế sài gòn online bộ trưởng -99,98 bộ giao thông vận tải hồ nghĩa dũng đã ký quyết định phê duyệt quy hoạch sân 889 l bay tại tỉnh an giang giai đoạn đến 14h53 và định hướng đến mồng 1/7 có tổng nhu cầu vốn dự kiến là 89368137454 tỉ đồng bình nguyên dự án sân bay tại xã cần đăng huyện châu thành sẽ được thực hiện trong ngày 19 và ngày 22 tháng 2 theo đó quy hoạch khu vực sân\n"
     ]
    }
   ],
   "source": [
    "SRC_MAX_LENGTH = 100\n",
    "TGT_MAX_LENGTH = 100\n",
    "\n",
    "file = \"input.txt\"\n",
    "with open(file, mode='r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Text input: \")\n",
    "print(text)\n",
    "print(100*\"-\")\n",
    "\n",
    "input_ids = tokenizer(text, max_length=SRC_MAX_LENGTH, truncation=True, return_tensors='pt')[\"input_ids\"]\n",
    "# generate text without beam-search\n",
    "outputs = trained_model.generate(\n",
    "    input_ids, \n",
    "    max_length=SRC_MAX_LENGTH, \n",
    "    num_return_sequences=1, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Text predict: \")\n",
    "for i, output in enumerate(outputs):\n",
    "    output_pieces = tokenizer.convert_ids_to_tokens(output.numpy().tolist())\n",
    "    output_text = tokenizer.sp_model.decode(output_pieces)\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thời báo kinh tế sài gòn online bộ trưởng âm chín chín phẩy chín tám bộ giao thông vận tải hồ nghĩa dũng đã ký quyết định phê duyệt quy hoạch sân tám trăm tám mươi chín lít bay tại tỉnh an giang giai đoạn đến mười bốn giờ năm mươi ba và định hướng đến mồng một tháng bảy có tổng nhu cầu vốn dự kiến là tám chín ba sáu tám một ba bẩy bốn năm bốn tỉ đồng bình nguyên dự án sân bay tại xã cần đăng huyện châu thành sẽ được thực hiện thành một\n",
      "ngàn tám trăm ba mươi bẩy giai đoạn với số vốn cần cho giai đoạn đến bốn giờ không phút là âm tám tám chấm không năm mươi tỉ đồng và giai đoạn định hướng đến một nghìn sáu trăm là âm chín ba ngàn hai trăm năm bốn phẩy bốn nghìn chín mươi bẩy tỉ đồng theo quyết định phê duyệt quy hoạch số i gi gi rờ lờ chín không không đến ngày mười hai sân bay an giang sẽ có một đường hạ cất cánh dài hai mươi ba ngàn bẩy trăm mười bốn phẩy ba nghìn\n",
      "tám trăm sáu mươi chín mét và rộng cộng bốn hai sáu sáu sáu không năm bốn sáu một mét đảm bảo cho các hoạt động khai thác của máy bay chín trăm một trăm hai mươi trừ rờ đờ ngang gi hoặc tương đương đường băng sẽ được nâng cấp để khả năng tiếp nhận máy bay airbus hai trăm gạch chéo tám một không xuộc đê quờ o e dét cho giai đoạn sau ngày bẩy dự kiến lượng hành khách tại sân bay này là sáu triệu một trăm bốn mươi bảy ngàn bốn trăm năm mươi\n",
      "hai khách/giờ cao điểm và lượng hàng hóa là hai triệu không ngàn hai trăm tấn/năm ngoài ra dự án sân bay có tổng diện tích hơn hai nhăm héc-ta này cũng sẽ bao gồm nhà dịch vụ hành khách rộng sáu trăm năm mươi chín mi li mét vuông trung tâm điều hành bay ngày hai mươi bảy và ngày hai mươi chín trạm xe dẫn đường và các công trình kỹ thuật hỗ trợ cho các hoạt động hạ cất cánh đường sân đỗ ô tô trước nhà ga sau không giờ nhà ga sẽ được nâng cấp\n",
      "và mở rộng đến năm mươi ba xen ti mét trên xen ti mét khối\n"
     ]
    }
   ],
   "source": [
    "file = \"input.txt\"\n",
    "with open(file, mode='r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "n = 100\n",
    "text = text.split(\" \")\n",
    "new = [text[i:i+n] for i in range(0, len(text), n)]\n",
    "for s in new:\n",
    "    s = ' '.join(s)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d49c3f6d6dd49f9272b571d9fad348ab55b8c6c3f691520d74ed0af1f69c3dd8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
